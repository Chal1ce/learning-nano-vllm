# ç¬¬äºŒç« ï¼šé¡¹ç›®ç»“æ„åˆ†æ

## ğŸ“š æœ¬ç« ç›®æ ‡

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å°†æ·±å…¥ç†è§£ï¼š
- nano-vLLMçš„å®Œæ•´ç›®å½•ç»“æ„
- æ¯ä¸ªæ–‡ä»¶å’Œæ¨¡å—çš„å…·ä½“èŒè´£
- å…³é”®ä»£ç çš„é€è¡Œåˆ†æ
- æ¨¡å—é—´çš„ä¾èµ–å…³ç³»å’Œæ•°æ®æµ
- é¡¹ç›®çš„å¯åŠ¨æµç¨‹å’Œåˆå§‹åŒ–è¿‡ç¨‹

---

## ğŸ—‚ï¸ é¡¹ç›®å®Œæ•´ç›®å½•ç»“æ„

é¦–å…ˆè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹é¡¹ç›®çš„å®Œæ•´ç›®å½•æ ‘ï¼š

```
nano-vllm-main/
â”œâ”€â”€ nanovllm/                    # ä¸»åŒ…ç›®å½•
â”‚   â”œâ”€â”€ __init__.py             # åŒ…åˆå§‹åŒ–æ–‡ä»¶ï¼Œå®šä¹‰å¯¹å¤–æ¥å£
â”‚   â”œâ”€â”€ config.py               # é…ç½®ç±»å®šä¹‰
â”‚   â”œâ”€â”€ llm.py                  # ä¸»LLMç±»ï¼Œç”¨æˆ·å…¥å£
â”‚   â”œâ”€â”€ sampling_params.py      # é‡‡æ ·å‚æ•°ç±»
â”‚   â”œâ”€â”€ engine/                 # æ¨ç†å¼•æ“æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ block_manager.py    # KV Cacheå—ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ llm_engine.py       # LLMå¼•æ“æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ model_runner.py     # æ¨¡å‹è¿è¡Œå™¨
â”‚   â”‚   â”œâ”€â”€ scheduler.py        # è¯·æ±‚è°ƒåº¦å™¨
â”‚   â”‚   â””â”€â”€ sequence.py         # åºåˆ—ç®¡ç†
â”‚   â”œâ”€â”€ models/                 # æ¨¡å‹å®ç°
â”‚   â”‚   â””â”€â”€ qwen3.py           # Qwen3æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ layers/                 # åŸºç¡€å±‚å®ç°
â”‚   â”‚   â”œâ”€â”€ activation.py       # æ¿€æ´»å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ attention.py       # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”‚   â”œâ”€â”€ embed_head.py      # åµŒå…¥å’Œè¾“å‡ºå¤´
â”‚   â”‚   â”œâ”€â”€ layernorm.py       # å±‚å½’ä¸€åŒ–
â”‚   â”‚   â”œâ”€â”€ linear.py          # çº¿æ€§å±‚
â”‚   â”‚   â”œâ”€â”€ rotary_embedding.py # æ—‹è½¬ä½ç½®ç¼–ç 
â”‚   â”‚   â””â”€â”€ sampler.py         # é‡‡æ ·å™¨
â”‚   â””â”€â”€ utils/                  # å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ context.py          # ä¸Šä¸‹æ–‡ç®¡ç†
â”‚       â””â”€â”€ loader.py           # æ¨¡å‹åŠ è½½å™¨
â”œâ”€â”€ UnderstandArch/             # æœ¬æ–‡æ¡£ç›®å½•
â”œâ”€â”€ assets/                     # èµ„æºæ–‡ä»¶
â”‚   â””â”€â”€ logo.png               # é¡¹ç›®Logo
â”œâ”€â”€ bench.py                   # æ€§èƒ½æµ‹è¯•è„šæœ¬
â”œâ”€â”€ example.py                 # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ LICENSE                    # å¼€æºè®¸å¯è¯
â”œâ”€â”€ pyproject.toml             # é¡¹ç›®é…ç½®æ–‡ä»¶
â””â”€â”€ README.md                  # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

---

## ğŸ”§ æ ¸å¿ƒé…ç½®æ–‡ä»¶åˆ†æ

### pyproject.toml - é¡¹ç›®å…ƒæ•°æ®å’Œä¾èµ–

```toml
[build-system]
requires = ["setuptools>=61"]
build-backend = "setuptools.build_meta"

[project]
name = "nano-vllm"
version = "0.2.0"
authors = [{ name = "Xingkai Yu" }]
license = "MIT"
license-files = ["LICENSE"]
readme = "README.md"
description = "a lightweight vLLM implementation built from scratch"
requires-python = ">=3.10,<3.13"
dependencies = [
    "torch>=2.4.0",          # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
    "triton>=3.0.0",         # GPUæ ¸å‡½æ•°ä¼˜åŒ–åº“
    "transformers>=4.51.0",  # HuggingFaceæ¨¡å‹åº“
    "flash-attn",            # Flash Attentionä¼˜åŒ–
    "xxhash",                # å¿«é€Ÿå“ˆå¸Œç®—æ³•
]
```

**é€è¡Œåˆ†æ**ï¼š

1. **`torch>=2.4.0`**:
   - PyTorchæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ¡†æ¶
   - éœ€è¦é«˜ç‰ˆæœ¬æ”¯æŒæœ€æ–°çš„GPUä¼˜åŒ–ç‰¹æ€§

2. **`triton>=3.0.0`**:
   - OpenAIå¼€å‘çš„GPUç¼–ç¨‹è¯­è¨€
   - ç”¨äºç¼–å†™é«˜æ•ˆçš„GPUæ ¸å‡½æ•°
   - nano-vLLMç”¨å®ƒæ¥ä¼˜åŒ–æŸäº›è®¡ç®—æ“ä½œ

3. **`flash-attn`**:
   - Flash Attentionå®ç°
   - å¤§å¹…æå‡æ³¨æ„åŠ›è®¡ç®—æ•ˆç‡
   - å‡å°‘å†…å­˜ä½¿ç”¨

4. **`xxhash`**:
   - æå¿«çš„éåŠ å¯†å“ˆå¸Œç®—æ³•
   - ç”¨äºPrefix Cachingä¸­çš„å“ˆå¸Œè®¡ç®—

---

## ğŸ“¦ ä¸»åŒ…å…¥å£åˆ†æ

### nanovllm/__init__.py - åŒ…çš„å…¬å…±æ¥å£

```python
from nanovllm.llm import LLM
from nanovllm.sampling_params import SamplingParams
```

**ä»£ç åˆ†æ**ï¼š
- è¿™åªæœ‰ä¸¤è¡Œä»£ç ï¼Œä½†éå¸¸å…³é”®
- å®šä¹‰äº†åŒ…çš„å…¬å…±æ¥å£ï¼Œç”¨æˆ·åªéœ€è¦å¯¼å…¥è¿™ä¸¤ä¸ªç±»
- `LLM`: ä¸»è¦çš„æ¨ç†å¼•æ“ç±»
- `SamplingParams`: é‡‡æ ·å‚æ•°é…ç½®ç±»

**è®¾è®¡æ€æƒ³**ï¼šéšè—å†…éƒ¨å¤æ‚æ€§ï¼Œç”¨æˆ·åªéœ€è¦çŸ¥é“è¿™ä¸¤ä¸ªç±»å°±èƒ½ä½¿ç”¨æ•´ä¸ªç³»ç»Ÿã€‚

### nanovllm/llm.py - ç”¨æˆ·ä¸»å…¥å£

```python
from nanovllm.engine.llm_engine import LLMEngine


class LLM(LLMEngine):
    pass
```

**ä»£ç åˆ†æ**ï¼š
- çœ‹èµ·æ¥å¾ˆç®€å•ï¼Œåªæ˜¯ç»§æ‰¿äº†`LLMEngine`
- å®é™…ä¸Šè¿™æ˜¯ä¸€ä¸ª"å¤–è§‚æ¨¡å¼"è®¾è®¡
- ç”¨æˆ·ç›´æ¥ä½¿ç”¨`LLM`ç±»ï¼Œä½†å®é™…åŠŸèƒ½éƒ½åœ¨`LLMEngine`ä¸­å®ç°
- è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼š
  - ç®€åŒ–ç”¨æˆ·æ¥å£
  - å°†å®ç°ç»†èŠ‚å°è£…åœ¨å†…éƒ¨
  - ä¾¿äºå°†æ¥æ‰©å±•æˆ–é‡æ„

---

## âš™ï¸ é…ç½®ç³»ç»Ÿæ·±åº¦è§£æ

### nanovllm/config.py - é…ç½®ç®¡ç†

```python
import os
from dataclasses import dataclass
from transformers import AutoConfig


@dataclass
class Config:
    model: str                                      # æ¨¡å‹è·¯å¾„
    max_num_batched_tokens: int = 16384            # æœ€å¤§æ‰¹å¤„ç†tokenæ•°
    max_num_seqs: int = 512                         # æœ€å¤§å¹¶å‘åºåˆ—æ•°
    max_model_len: int = 4096                       # æ¨¡å‹æœ€å¤§é•¿åº¦
    gpu_memory_utilization: float = 0.9            # GPUå†…å­˜ä½¿ç”¨ç‡
    tensor_parallel_size: int = 1                   # å¼ é‡å¹¶è¡Œå¤§å°
    enforce_eager: bool = False                     # å¼ºåˆ¶eageræ¨¡å¼
    hf_config: AutoConfig | None = None            # HuggingFaceé…ç½®
    eos: int = -1                                   # ç»“æŸtoken ID
    kvcache_block_size: int = 256                   # KV Cacheå—å¤§å°
    num_kvcache_blocks: int = -1                   # KV Cacheå—æ•°é‡

    def __post_init__(self):
        assert os.path.isdir(self.model)            # ç¡®ä¿æ¨¡å‹è·¯å¾„å­˜åœ¨
        assert self.kvcache_block_size % 256 == 0   # å—å¤§å°å¿…é¡»æ˜¯256çš„å€æ•°
        assert 1 <= self.tensor_parallel_size <= 8  # å¹¶è¡Œå¤§å°é™åˆ¶
        self.hf_config = AutoConfig.from_pretrained(self.model)  # åŠ è½½HFé…ç½®
        self.max_model_len = min(self.max_model_len, self.hf_config.max_position_embeddings)  # è°ƒæ•´æœ€å¤§é•¿åº¦
        assert self.max_num_batched_tokens >= self.max_model_len  # æ‰¹å¤„ç†å¤§å°é™åˆ¶
```

**å…³é”®å‚æ•°è¯¦è§£**ï¼š

1. **`max_num_batched_tokens = 16384`**
   - ä¸€æ¬¡æœ€å¤šå¤„ç†å¤šå°‘ä¸ªtoken
   - å½±å“å†…å­˜ä½¿ç”¨å’Œå¤„ç†é€Ÿåº¦
   - è¶Šå¤§è¶Šå¥½ï¼Œä½†å—GPUå†…å­˜é™åˆ¶

2. **`max_num_seqs = 512`**
   - åŒæ—¶å¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°
   - å½±å“å¹¶å‘èƒ½åŠ›
   - éœ€è¦åœ¨å†…å­˜å’Œå»¶è¿Ÿé—´å¹³è¡¡

3. **`kvcache_block_size = 256`**
   - KV Cacheçš„å—å¤§å°
   - 256æ˜¯ä¸€ä¸ªç»éªŒæœ€ä¼˜å€¼
   - å½±å“å†…å­˜åˆ†é…æ•ˆç‡

4. **`tensor_parallel_size`**
   - å¼ é‡å¹¶è¡Œçš„GPUæ•°é‡
   - 1è¡¨ç¤ºå•GPU
   - 8è¡¨ç¤ºæœ€å¤š8ä¸ªGPUå¹¶è¡Œ

**é…ç½®éªŒè¯é€»è¾‘**ï¼š
- `__post_init__`æ–¹æ³•åœ¨å¯¹è±¡åˆ›å»ºåè‡ªåŠ¨è°ƒç”¨
- è¿›è¡Œå„ç§å‚æ•°åˆæ³•æ€§æ£€æŸ¥
- è‡ªåŠ¨åŠ è½½HuggingFaceé…ç½®
- è°ƒæ•´ä¸åˆç†çš„å‚æ•°å€¼

---

## ğŸ² é‡‡æ ·å‚æ•°ç³»ç»Ÿ

### nanovllm/sampling_params.py - é‡‡æ ·é…ç½®

```python
from dataclasses import dataclass


@dataclass
class SamplingParams:
    temperature: float = 1.0                         # æ¸©åº¦å‚æ•°
    max_tokens: int = 64                            # æœ€å¤§ç”Ÿæˆtokenæ•°
    ignore_eos: bool = False                        # æ˜¯å¦å¿½ç•¥ç»“æŸtoken

    def __post_init__(self):
        assert self.temperature > 1e-10, "greedy sampling is not permitted"
```

**å‚æ•°è¯¦è§£**ï¼š

1. **`temperature`**
   - æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§
   - 1.0: æ ‡å‡†éšæœºæ€§
   - 0.1: è¶‹å‘ç¡®å®šæ€§ï¼ˆç±»ä¼¼è´ªå¿ƒé‡‡æ ·ï¼‰
   - 2.0: é«˜éšæœºæ€§ï¼ˆåˆ›æ„æ€§å¼ºï¼‰
   - ä¸å…è®¸æ¥è¿‘0ï¼Œå› ä¸ºnano-vLLMä¸æ”¯æŒçº¯è´ªå¿ƒé‡‡æ ·

2. **`max_tokens`**
   - é™åˆ¶ç”Ÿæˆé•¿åº¦
   - é˜²æ­¢æ— é™ç”Ÿæˆ
   - éœ€è¦æƒè¡¡å“åº”é•¿åº¦å’Œå»¶è¿Ÿ

3. **`ignore_eos`**
   - æ˜¯å¦å¿½ç•¥ç»“æŸtoken
   - True: å¼ºåˆ¶ç”ŸæˆæŒ‡å®šé•¿åº¦
   - False: é‡åˆ°ç»“æŸtokenå°±åœæ­¢

---

## ğŸš€ å¼•æ“æ¨¡å—é€è¡Œåˆ†æ

### nanovllm/engine/sequence.py - åºåˆ—ç®¡ç†

```python
from copy import copy
from enum import Enum, auto
from itertools import count

from nanovllm.sampling_params import SamplingParams


class SequenceStatus(Enum):
    WAITING = auto()    # ç­‰å¾…å¤„ç†
    RUNNING = auto()    # æ­£åœ¨å¤„ç†
    FINISHED = auto()   # å·²å®Œæˆ


class Sequence:
    block_size = 256                    # å—å¤§å°å¸¸é‡
    counter = count()                  # å…¨å±€è®¡æ•°å™¨ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€ID

    def __init__(self, token_ids: list[int], sampling_params = SamplingParams()):
        self.seq_id = next(Sequence.counter)           # ç”Ÿæˆå”¯ä¸€åºåˆ—ID
        self.status = SequenceStatus.WAITING           # åˆå§‹çŠ¶æ€ä¸ºç­‰å¾…
        self.token_ids = copy(token_ids)               # å¤åˆ¶è¾“å…¥tokenåˆ—è¡¨
        self.last_token = token_ids[-1]                # æœ€åä¸€ä¸ªtoken
        self.num_tokens = len(token_ids)               # æ€»tokenæ•°
        self.num_prompt_tokens = len(token_ids)        # æç¤ºè¯tokenæ•°
        self.num_cached_tokens = 0                     # å·²ç¼“å­˜tokenæ•°
        self.block_table = []                          # KV Cacheå—è¡¨
        self.temperature = sampling_params.temperature  # é‡‡æ ·æ¸©åº¦
        self.max_tokens = sampling_params.max_tokens    # æœ€å¤§ç”Ÿæˆé•¿åº¦
        self.ignore_eos = sampling_params.ignore_eos    # æ˜¯å¦å¿½ç•¥EOS

    def __len__(self):
        return self.num_tokens

    def __getitem__(self, key):
        return self.token_ids[key]

    @property
    def is_finished(self):
        return self.status == SequenceStatus.FINISHED

    @property
    def num_completion_tokens(self):
        return self.num_tokens - self.num_prompt_tokens

    @property
    def completion_token_ids(self):
        return self.token_ids[self.num_prompt_tokens:]
```

**å…³é”®è®¾è®¡ç‚¹åˆ†æ**ï¼š

1. **çŠ¶æ€ç®¡ç†**ï¼š
   - ä½¿ç”¨æšä¸¾å®šä¹‰ä¸‰ç§çŠ¶æ€
   - è·Ÿè¸ªåºåˆ—çš„ç”Ÿå‘½å‘¨æœŸ

2. **å”¯ä¸€IDç”Ÿæˆ**ï¼š
   - ä½¿ç”¨`itertools.count()`ç”Ÿæˆå…¨å±€å”¯ä¸€ID
   - é¿å…IDå†²çª

3. **Tokenç®¡ç†**ï¼š
   - åŒºåˆ†æç¤ºè¯tokenå’Œç”Ÿæˆtoken
   - è·Ÿè¸ªç¼“å­˜çŠ¶æ€

4. **å†…å­˜ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨`copy()`é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
   - å—è¡¨ç®¡ç†KV Cacheå¼•ç”¨

### nanovllm/engine/block_manager.py - KV Cacheç®¡ç†å™¨

```python
from collections import deque
import xxhash
import numpy as np

from nanovllm.engine.sequence import Sequence


class Block:
    def __init__(self, block_id):
        self.block_id = block_id
        self.ref_count = 0          # å¼•ç”¨è®¡æ•°
        self.hash = -1              # å†…å®¹å“ˆå¸Œ
        self.token_ids = []         # å­˜å‚¨çš„tokenåˆ—è¡¨

    def update(self, hash: int, token_ids: list[int]):
        self.hash = hash
        self.token_ids = token_ids

    def reset(self):
        self.ref_count = 1
        self.hash = -1
        self.token_ids = []


class BlockManager:
    def __init__(self, num_blocks: int, block_size: int):
        self.block_size = block_size
        self.blocks: list[Block] = [Block(i) for i in range(num_blocks)]
        self.hash_to_block_id: dict[int, int] = dict()
        self.free_block_ids: deque[int] = deque(range(num_blocks))
        self.used_block_ids: set[int] = set()

    @classmethod
    def compute_hash(cls, token_ids: list[int], prefix: int = -1):
        h = xxhash.xxh64()
        if prefix != -1:
            h.update(prefix.to_bytes(8, "little"))
        h.update(np.array(token_ids).tobytes())
        return h.intdigest()
```

**BlockManagerçš„æ ¸å¿ƒåŠŸèƒ½**ï¼š

1. **å—åˆ†é…**ï¼šç®¡ç†ç©ºé—²å—å’Œå·²ä½¿ç”¨å—
2. **å“ˆå¸Œç¼“å­˜**ï¼šé€šè¿‡å“ˆå¸Œå®ç°Prefix Caching
3. **å¼•ç”¨è®¡æ•°**ï¼šè·Ÿè¸ªå—çš„å¼•ç”¨æƒ…å†µ

**Prefix CachingåŸç†**ï¼š
- ç›¸åŒå‰ç¼€çš„è¯·æ±‚å¯ä»¥å…±äº«KV Cache
- ä½¿ç”¨å“ˆå¸Œå¿«é€Ÿè¯†åˆ«ç›¸åŒå‰ç¼€
- å¤§å¹…å‡å°‘é‡å¤è®¡ç®—

### nanovllm/engine/scheduler.py - è¯·æ±‚è°ƒåº¦å™¨

```python
from collections import deque

from nanovllm.config import Config
from nanovllm.engine.sequence import Sequence, SequenceStatus
from nanovllm.engine.block_manager import BlockManager


class Scheduler:
    def __init__(self, config: Config):
        self.max_num_seqs = config.max_num_seqs
        self.max_num_batched_tokens = config.max_num_batched_tokens
        self.eos = config.eos
        self.block_manager = BlockManager(config.num_kvcache_blocks, config.kvcache_block_size)
        self.waiting: deque[Sequence] = deque()      # ç­‰å¾…é˜Ÿåˆ—
        self.running: deque[Sequence] = deque()      # è¿è¡Œé˜Ÿåˆ—

    def is_finished(self):
        return not self.waiting and not self.running

    def add(self, seq: Sequence):
        self.waiting.append(seq)

    def schedule(self) -> tuple[list[Sequence], bool]:
        # Prefillé˜¶æ®µï¼šå¤„ç†æ–°è¯·æ±‚
        scheduled_seqs = []
        num_seqs = 0
        num_batched_tokens = 0

        while self.waiting and num_seqs < self.max_num_seqs:
            seq = self.waiting[0]
            if num_batched_tokens + len(seq) > self.max_num_batched_tokens or not self.block_manager.can_allocate(seq):
                break

            num_seqs += 1
            self.block_manager.allocate(seq)
            num_batched_tokens += len(seq) - seq.num_cached_tokens
            seq.status = SequenceStatus.RUNNING
            self.waiting.popleft()
            self.running.append(seq)
            scheduled_seqs.append(seq)

        if scheduled_seqs:
            return scheduled_seqs, True

        # Decodeé˜¶æ®µï¼šå¤„ç†è¿è¡Œä¸­çš„è¯·æ±‚
        while self.running and num_seqs < self.max_num_seqs:
            seq = self.running.popleft()

            while not self.block_manager.can_append(seq):
                if self.running:
                    self.preempt(self.running.pop())
                else:
                    self.preempt(seq)
                    break
            else:
                num_seqs += 1
                self.running.append(seq)
                scheduled_seqs.append(seq)

        return scheduled_seqs, False
```

**è°ƒåº¦ç­–ç•¥åˆ†æ**ï¼š

1. **ä¸¤é˜¶æ®µè°ƒåº¦**ï¼š
   - **Prefillé˜¶æ®µ**ï¼šå¤„ç†æ–°è¯·æ±‚çš„åˆå§‹è®¡ç®—
   - **Decodeé˜¶æ®µ**ï¼šå¤„ç†å·²è¿è¡Œè¯·æ±‚çš„å¢é‡è®¡ç®—

2. **èµ„æºé™åˆ¶**ï¼š
   - `max_num_seqs`ï¼šé™åˆ¶æœ€å¤§å¹¶å‘æ•°
   - `max_num_batched_tokens`ï¼šé™åˆ¶æ‰¹å¤„ç†å¤§å°

3. **æŠ¢å æœºåˆ¶**ï¼š
   - å½“å†…å­˜ä¸è¶³æ—¶ï¼ŒæŠ¢å é•¿åºåˆ—çš„èµ„æº
   - ä¿è¯çŸ­åºåˆ—èƒ½å¤Ÿç»§ç»­å¤„ç†

---

## ğŸ§  æ¨¡å‹è¿è¡Œå™¨æ·±åº¦è§£æ

### nanovllm/engine/model_runner.py - æ¨¡å‹æ‰§è¡Œæ ¸å¿ƒ

è®©æˆ‘ä»¬æ·±å…¥åˆ†æè¿™ä¸ªæœ€é‡è¦çš„æ–‡ä»¶ï¼š

```python
import pickle
import torch
import torch.distributed as dist
from multiprocessing.synchronize import Event
from multiprocessing.shared_memory import SharedMemory

from nanovllm.config import Config
from nanovllm.engine.sequence import Sequence
from nanovllm.models.qwen3 import Qwen3ForCausalLM
from nanovllm.layers.sampler import Sampler
from nanovllm.utils.context import set_context, get_context, reset_context
from nanovllm.utils.loader import load_model


class ModelRunner:
    def __init__(self, config: Config, rank: int, event: Event | list[Event]):
        # é…ç½®å’Œè®¾å¤‡è®¾ç½®
        self.config = config
        hf_config = config.hf_config
        self.block_size = config.kvcache_block_size
        self.enforce_eager = config.enforce_eager
        self.world_size = config.tensor_parallel_size
        self.rank = rank
        self.event = event

        # åˆå§‹åŒ–åˆ†å¸ƒå¼é€šä¿¡
        dist.init_process_group("nccl", "tcp://localhost:2333", world_size=self.world_size, rank=rank)
        torch.cuda.set_device(rank)

        # è®¾ç½®é»˜è®¤æ•°æ®ç±»å‹
        default_dtype = torch.get_default_dtype()
        torch.set_default_dtype(hf_config.torch_dtype)
        torch.set_default_device("cuda")

        # åˆ›å»ºæ¨¡å‹å’Œé‡‡æ ·å™¨
        self.model = Qwen3ForCausalLM(hf_config)
        load_model(self.model, config.model)
        self.sampler = Sampler()

        # æ¨¡å‹ä¼˜åŒ–
        self.warmup_model()
        self.allocate_kv_cache()
        if not self.enforce_eager:
            self.capture_cudagraph()

        # æ¢å¤é»˜è®¤è®¾ç½®
        torch.set_default_device("cpu")
        torch.set_default_dtype(default_dtype)

        # å¤šè¿›ç¨‹å¤„ç†
        if self.world_size > 1:
            if rank == 0:
                self.shm = SharedMemory(name="nanovllm", create=True, size=2**20)
                dist.barrier()
            else:
                dist.barrier()
                self.shm = SharedMemory(name="nanovllm")
                self.loop()
```

**åˆå§‹åŒ–è¿‡ç¨‹è¯¦è§£**ï¼š

1. **åˆ†å¸ƒå¼è®¾ç½®**ï¼š
   ```python
   dist.init_process_group("nccl", "tcp://localhost:2333", world_size=self.world_size, rank=rank)
   ```
   - NCCLæ˜¯NVIDIAçš„é«˜æ•ˆé€šä¿¡åº“
   - ç”¨äºå¤šGPUé—´çš„å¼ é‡é€šä¿¡
   - å›ºå®šç«¯å£2333ç”¨äºè¿›ç¨‹é—´é€šä¿¡

2. **è®¾å¤‡ç®¡ç†**ï¼š
   ```python
   torch.cuda.set_device(rank)
   ```
   - æ¯ä¸ªè¿›ç¨‹ç»‘å®šåˆ°ç‰¹å®šçš„GPU
   - rank 0ç»‘å®šåˆ°GPU 0ï¼Œrank 1ç»‘å®šåˆ°GPU 1

3. **æ¨¡å‹åŠ è½½**ï¼š
   ```python
   self.model = Qwen3ForCausalLM(hf_config)
   load_model(self.model, config.model)
   ```
   - åˆ›å»ºæ¨¡å‹å®ä¾‹
   - ä»æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæƒé‡

4. **CUDA Graphä¼˜åŒ–**ï¼š
   ```python
   if not self.enforce_eager:
       self.capture_cudagraph()
   ```
   - æ•è·è®¡ç®—å›¾ï¼Œé¿å…é‡å¤çš„å¼€é”€
   - å¤§å¹…æå‡æ¨ç†é€Ÿåº¦

---

## ğŸ”§ å·¥å…·æ¨¡å—åˆ†æ

### nanovllm/utils/context.py - ä¸Šä¸‹æ–‡ç®¡ç†

```python
import threading
from typing import Any, Dict

_context = threading.local()


def set_context(**kwargs):
    """è®¾ç½®ä¸Šä¸‹æ–‡å˜é‡"""
    for key, value in kwargs.items():
        setattr(_context, key, value)


def get_context(key: str, default: Any = None) -> Any:
    """è·å–ä¸Šä¸‹æ–‡å˜é‡"""
    return getattr(_context, key, default)


def reset_context():
    """é‡ç½®ä¸Šä¸‹æ–‡"""
    _context.__dict__.clear()


class Context:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""

    def __init__(self, **kwargs):
        self.kwargs = kwargs
        self.old_context = {}

    def __enter__(self):
        # ä¿å­˜æ—§çš„ä¸Šä¸‹æ–‡
        for key, value in self.kwargs.items():
            self.old_context[key] = get_context(key)
            set_context(**{key: value})
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # æ¢å¤æ—§çš„ä¸Šä¸‹æ–‡
        reset_context()
        for key, value in self.old_context.items():
            if value is not None:
                set_context(**{key: value})
```

**ä¸Šä¸‹æ–‡ç³»ç»Ÿçš„ä½œç”¨**ï¼š
- çº¿ç¨‹æœ¬åœ°å­˜å‚¨ï¼Œé¿å…å¤šçº¿ç¨‹å†²çª
- ç®¡ç†å…¨å±€é…ç½®å’ŒçŠ¶æ€
- æ”¯æŒä¸´æ—¶ä¿®æ”¹é…ç½®

### nanovllm/utils/loader.py - æ¨¡å‹åŠ è½½å™¨

```python
import torch
from typing import Dict, Any


def load_model(model: torch.nn.Module, model_path: str) -> None:
    """åŠ è½½æ¨¡å‹æƒé‡"""
    state_dict = torch.load(f"{model_path}/pytorch_model.bin", map_location="cpu")
    model.load_state_dict(state_dict, strict=True)
    print(f"Loaded model from {model_path}")
```

**åŠ è½½è¿‡ç¨‹**ï¼š
- ä»ç£ç›˜åŠ è½½æƒé‡æ–‡ä»¶
- ä½¿ç”¨CPUåŠ è½½é¿å…å†…å­˜çˆ†ç‚¸
- ä¸¥æ ¼åŒ¹é…æƒé‡åç§°

---

## ğŸš€ é¡¹ç›®å¯åŠ¨æµç¨‹åˆ†æ

### å®Œæ•´çš„åˆå§‹åŒ–åºåˆ—

è®©æˆ‘ä»¬é€šè¿‡è·Ÿè¸ªä¸€ä¸ªç®€å•çš„ä½¿ç”¨ç¤ºä¾‹æ¥ç†è§£å¯åŠ¨æµç¨‹ï¼š

```python
from nanovllm import LLM, SamplingParams

# 1. åˆ›å»ºLLMå®ä¾‹
llm = LLM("/path/to/model", tensor_parallel_size=1)
```

**èƒŒåå‘ç”Ÿçš„äº‹æƒ…**ï¼š

1. **é…ç½®è§£æ**ï¼š
   ```python
   # LLM.__init__ è°ƒç”¨ LLMEngine.__init__
   config = Config(model, **kwargs)  # è§£æå¹¶éªŒè¯é…ç½®
   ```

2. **å¤šè¿›ç¨‹åˆå§‹åŒ–**ï¼š
   ```python
   ctx = mp.get_context("spawn")
   for i in range(1, config.tensor_parallel_size):  # å¯¹äºæ¯ä¸ªé¢å¤–GPU
       event = ctx.Event()
       process = ctx.Process(target=ModelRunner, args=(config, i, event))
       process.start()
   ```

3. **ä¸»è¿›ç¨‹ModelRunneråˆå§‹åŒ–**ï¼š
   ```python
   self.model_runner = ModelRunner(config, 0, self.events)
   ```

4. **æ¨¡å‹åŠ è½½å’Œä¼˜åŒ–**ï¼š
   ```python
   # åœ¨ModelRunner.__init__ä¸­
   self.model = Qwen3ForCausalLM(hf_config)
   load_model(self.model, config.model)
   self.warmup_model()
   self.allocate_kv_cache()
   self.capture_cudagraph()  # å¦‚æœå¯ç”¨
   ```

5. **è°ƒåº¦å™¨åˆå§‹åŒ–**ï¼š
   ```python
   self.scheduler = Scheduler(config)
   ```

### ç”Ÿæˆè¿‡ç¨‹çš„è°ƒç”¨é“¾

```python
# 2. ç”Ÿæˆæ–‡æœ¬
outputs = llm.generate(prompts, sampling_params)
```

**è°ƒç”¨é“¾åˆ†æ**ï¼š

1. **LLMEngine.generate()**ï¼š
   ```python
   # æ·»åŠ æ‰€æœ‰è¯·æ±‚åˆ°è°ƒåº¦å™¨
   for prompt, sp in zip(prompts, sampling_params):
       self.add_request(prompt, sp)

   # ä¸»æ¨ç†å¾ªç¯
   while not self.is_finished():
       output, num_tokens = self.step()  # æ‰§è¡Œä¸€æ­¥æ¨ç†
       # å¤„ç†è¾“å‡º...
   ```

2. **LLMEngine.step()**ï¼š
   ```python
   # 1. è°ƒåº¦ï¼šå†³å®šå“ªäº›åºåˆ—è¦å¤„ç†
   seqs, is_prefill = self.scheduler.schedule()

   # 2. æ¨ç†ï¼šè®©æ¨¡å‹è®¡ç®—
   token_ids = self.model_runner.call("run", seqs, is_prefill)

   # 3. åå¤„ç†ï¼šæ›´æ–°åºåˆ—çŠ¶æ€
   self.scheduler.postprocess(seqs, token_ids)
   ```

3. **ModelRunner.run()**ï¼š
   ```python
   # 1. å‡†å¤‡è¾“å…¥æ•°æ®
   input_ids, position_ids, slot_mapping = self.prepare_inputs(seqs)

   # 2. æ¨¡å‹å‰å‘è®¡ç®—
   logits = self.model(input_ids, position_ids, slot_mapping)

   # 3. é‡‡æ ·ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
   next_token_ids = self.sampler(logits, temperatures)

   # 4. æ›´æ–°åºåˆ—
   self.update_sequences(seqs, next_token_ids)
   ```

---

## ğŸ“Š æ¨¡å—ä¾èµ–å…³ç³»å›¾

```
LLM (ç”¨æˆ·æ¥å£)
    â†“ ç»§æ‰¿
LLMEngine (å¼•æ“æ ¸å¿ƒ)
    â†“ ä½¿ç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scheduler (è°ƒåº¦å™¨)                                      â”‚
â”‚ â†“ ä½¿ç”¨ â†“ ä½¿ç”¨ â†“ ä½¿ç”¨                                   â”‚
â”‚ BlockManager â† Sequence â† ModelRunner                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ ä½¿ç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qwen3ForCausalLM (æ¨¡å‹)                                â”‚
â”‚ â†“ ä½¿ç”¨ â†“ ä½¿ç”¨                                          â”‚
â”‚ Attention, Linear, etc. (åŸºç¡€å±‚)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ ä½¿ç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context, Loader (å·¥å…·)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” ä»£ç å¯¼èˆªæŠ€å·§

### å¦‚ä½•å¿«é€Ÿæ‰¾åˆ°ç‰¹å®šåŠŸèƒ½

1. **ç”¨æˆ·å…¥å£**ï¼šä»`nanovllm/llm.py`å¼€å§‹
2. **æ ¸å¿ƒé€»è¾‘**ï¼š`nanovllm/engine/llm_engine.py`
3. **æ¨¡å‹å®šä¹‰**ï¼š`nanovllm/models/qwen3.py`
4. **æ€§èƒ½ä¼˜åŒ–**ï¼š`nanovllm/layers/`ç›®å½•
5. **é…ç½®ç®¡ç†**ï¼š`nanovllm/config.py`

### è°ƒè¯•å’Œåˆ†ææŠ€å·§

1. **æ·»åŠ æ—¥å¿—**ï¼š
   ```python
   print(f"[DEBUG] å½“å‰å¤„ç†åºåˆ—æ•°: {len(seqs)}")
   print(f"[DEBUG] å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1e9:.2f}GB")
   ```

2. **æ€§èƒ½åˆ†æ**ï¼š
   ```python
   import time
   start = time.time()
   # ... ä»£ç  ...
   print(f"è€—æ—¶: {time.time() - start:.3f}s")
   ```

3. **å†…å­˜æ£€æŸ¥**ï¼š
   ```python
   print(f"GPUå†…å­˜: {torch.cuda.max_memory_allocated() / 1e9:.2f}GB")
   ```

---

## ğŸ’¡ æœ¬ç« æ€»ç»“

### å…³é”®è¦ç‚¹å›é¡¾

1. **é¡¹ç›®ç»“æ„æ¸…æ™°**ï¼š
   - åˆ†å±‚è®¾è®¡ï¼ŒèŒè´£æ˜ç¡®
   - æ¯ä¸ªæ¨¡å—éƒ½æœ‰ç‰¹å®šçš„åŠŸèƒ½

2. **é…ç½®ç³»ç»Ÿå®Œå–„**ï¼š
   - ç»Ÿä¸€çš„é…ç½®ç®¡ç†
   - åˆç†çš„é»˜è®¤å€¼å’ŒéªŒè¯

3. **å¯åŠ¨æµç¨‹å¤æ‚ä½†æœ‰åº**ï¼š
   - å¤šè¿›ç¨‹åˆå§‹åŒ–
   - æ¨¡å‹åŠ è½½å’Œä¼˜åŒ–
   - è°ƒåº¦å™¨å‡†å¤‡

4. **è°ƒç”¨é“¾æ¸…æ™°**ï¼š
   - ç”¨æˆ·æ¥å£ â†’ å¼•æ“ â†’ è°ƒåº¦å™¨ â†’ æ¨¡å‹è¿è¡Œå™¨ â†’ æ¨¡å‹
   - æ¯ä¸€å±‚éƒ½æœ‰æ˜ç¡®çš„èŒè´£

### é‡è¦ä»£ç ä½ç½®

- **ä¸»å…¥å£**ï¼š`nanovllm/llm.py:4` - LLMç±»å®šä¹‰
- **å¼•æ“æ ¸å¿ƒ**ï¼š`nanovllm/engine/llm_engine.py:15` - LLMEngineç±»
- **è°ƒåº¦é€»è¾‘**ï¼š`nanovllm/engine/scheduler.py:24` - scheduleæ–¹æ³•
- **æ¨¡å‹è¿è¡Œ**ï¼š`nanovllm/engine/model_runner.py:17` - ModelRunnerç±»
- **åºåˆ—ç®¡ç†**ï¼š`nanovllm/engine/sequence.py:14` - Sequenceç±»

### ä¸‹ä¸€æ­¥é¢„å‘Š

ä¸‹ä¸€ç« æˆ‘ä»¬å°†æ·±å…¥åˆ†æã€Šæ ¸å¿ƒå¼•æ“æ¨¡å—ã€‹ï¼ŒåŒ…æ‹¬ï¼š
- LLMEngineçš„è¯¦ç»†å®ç°
- Schedulerçš„è°ƒåº¦ç®—æ³•
- ModelRunnerçš„æ‰§è¡Œæµç¨‹
- å„ç§ä¼˜åŒ–æŠ€æœ¯çš„å…·ä½“å®ç°

ç°åœ¨ä½ å¯¹nano-vLLMçš„æ•´ä½“ç»“æ„åº”è¯¥æœ‰äº†æ¸…æ™°çš„ç†è§£ï¼Œå‡†å¤‡å¥½è¿›å…¥æ›´æ·±å…¥çš„æŠ€æœ¯ç»†èŠ‚äº†å—ï¼ŸğŸš€